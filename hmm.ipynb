{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pomegranate import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Dirichlet, Normal\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # number of latent state\n",
    "M = 10  # length of observations\n",
    "N = 10  # number of observations\n",
    "\n",
    "mu = [np.log(z + 1) for z in range(K)]  # the mean of our observation model, i.e. E[p(x|z)]\n",
    "std = [1.] * K  # standard deviation on gaussian observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Parameters\n",
    "A = F.softmax(torch.randn(K, K), dim=-1)  # randomly initialize a transition matrix\n",
    "pi = F.softmax(torch.randn(K), dim=-1)  # randomly initialize a distribution over the initial latent state\n",
    "\n",
    "# Data\n",
    "X, Z = [], []\n",
    "for n in range(N):\n",
    "    obs = []\n",
    "    for m in range(M):\n",
    "        if m == 0:\n",
    "            z = np.random.choice(range(K), p=pi.numpy())\n",
    "        else:\n",
    "            z = np.random.choice(range(K), p=A[z].numpy())\n",
    "        x = mu[z] + std[z] * np.random.randn()\n",
    "        obs.append((z, x))\n",
    "    z_m, x_m = zip(*obs)\n",
    "    Z.append(z_m)\n",
    "    X.append(x_m)\n",
    "\n",
    "Z = np.array(Z); X = np.array(X)\n",
    "assert Z.shape == (N, M)\n",
    "assert X.shape == (N, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 0 | p(z_m):       2 | p(x_m|2): 0.296\n",
      "m: 1 | p(z_m|z_m-1): 0 | p(x_m|0): -0.449\n",
      "m: 2 | p(z_m|z_m-1): 1 | p(x_m|1): -0.413\n",
      "m: 3 | p(z_m|z_m-1): 4 | p(x_m|4): -0.0451\n",
      "m: 4 | p(z_m|z_m-1): 3 | p(x_m|3): 0.369\n",
      "m: 5 | p(z_m|z_m-1): 4 | p(x_m|4): 2.25\n",
      "m: 6 | p(z_m|z_m-1): 0 | p(x_m|0): -0.86\n",
      "m: 7 | p(z_m|z_m-1): 3 | p(x_m|3): 3.16\n",
      "m: 8 | p(z_m|z_m-1): 1 | p(x_m|1): 1.26\n",
      "m: 9 | p(z_m|z_m-1): 4 | p(x_m|4): 1.04\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "x, z = X[n], Z[n]\n",
    "\n",
    "for m, (z_m, x_m) in enumerate(zip(z, x)):\n",
    "    if m == 0:\n",
    "        print(f'm: {m} | p(z_m):       {z_m} | p(x_m|{z_m}): {x_m:1.3}')\n",
    "    else:\n",
    "        print(f'm: {m} | p(z_m|z_m-1): {z_m} | p(x_m|{z_m}): {x_m:1.3}')\n",
    "        \n",
    "del x, z, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \n",
    "    EMBED_DIM = 10\n",
    "    HIDDEN_DIM = 5\n",
    "    \n",
    "    def __init__(self, pi=None, A=None, mu=None, std=None, seed=1):\n",
    "        torch.manual_seed(seed)\n",
    "        self.A = A if A is not None else F.softmax(torch.randn(K, K), dim=-1)\n",
    "        self.pi = pi if pi is not None else F.softmax(torch.randn(K), dim=-1)\n",
    "        self.mu = mu if mu is not None else torch.randn(K).abs()\n",
    "        self.std = std if std is not None else torch.randn(K).abs()\n",
    "        \n",
    "    def loglik(self, z, x):\n",
    "        \"\"\"\n",
    "        The likelihood of the given observation `x`, conditional on latent code `z`, \n",
    "        given a Gaussian observation model.\n",
    "        \"\"\"\n",
    "        x = torch.FloatTensor([x])\n",
    "        return Normal(loc=self.mu[z], scale=self.std[z]).log_prob(x)\n",
    "\n",
    "    def factor(self, z_i, z_j, x):\n",
    "        return self.A[z_i][z_j] * self.loglik(z_j, x).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HMM factor is given by: $f(z_{n-1}, z_n) = p(z_n|z_n-1)p(X|z_n)$, where $n$ is\n",
    "our current index on the chain.\n",
    "\n",
    "The message from factor to variable, moving forward along the chain, is given by:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_{z_{n-1}} f(z_{n-1}, z_n) * \\mu_{f_{n-1} \\rightarrow z_{n-1}}\n",
    "$$\n",
    "\n",
    "In an efficient implementation, for each value $z_j$, we would compute $p(X|z_n)$ upfront then multiply it by the summation. Below, we recompute it inside each term of the summation unnecessarily for demonstrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_step(model, x):\n",
    "    alpha = []\n",
    "    for m in range(M):\n",
    "        if m == 0:\n",
    "            a_m = [model.pi[z] * model.loglik(z, x[m]).exp() for z in range(K)]\n",
    "\n",
    "        else:\n",
    "            a_m = [sum(model.factor(z_i, z_j, x[m]) * alpha[-1][z_i] for z_i in range(K)) for z_j in range(K)]\n",
    "        alpha.append(torch.FloatTensor(a_m))\n",
    "    return torch.stack(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_step(model, x):\n",
    "    beta = []\n",
    "    for m in reversed(range(M)):\n",
    "        if m == M - 1:\n",
    "            b_m = [1. for _ in range(K)]\n",
    "        else:\n",
    "            b_m = [sum(model.factor(z_i, z_j, x[m+1]) * beta[-1][z_j] for z_j in range(K)) for z_i in range(K)]\n",
    "        beta.append(torch.FloatTensor(b_m))\n",
    "    return torch.stack(beta).flip(0)  # NB: we flip the axes back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gamma(alpha, beta):\n",
    "    gamma_ = alpha * beta\n",
    "    evd = gamma_.sum(1)\n",
    "    # p(X) = \\sum_{z_n} \\alpha(z_n) * \\beta(z_n), for any choice of n!\n",
    "    assert np.allclose(evd[0], evd)\n",
    "    gamma = gamma_ / evd[0]\n",
    "    assert np.allclose(gamma.sum(1), 1.)\n",
    "    return gamma, evd[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zeta(model, alpha, beta, evd, x):\n",
    "    zeta_ = []\n",
    "    for m in range(M-1):\n",
    "        liks = np.array([model.loglik(z, x[m+1]).exp().item() for z in range(K)])\n",
    "        zeta_m = np.outer(alpha[m], beta[m+1]) * model.A.detach().numpy() * liks\n",
    "        zeta_.append(zeta_m)\n",
    "\n",
    "    zeta = torch.FloatTensor(zeta_).clone() / evd\n",
    "    assert all([np.allclose(zta.sum(), 1.) for zta in zeta])\n",
    "    \n",
    "    return zeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(model, x):\n",
    "    alpha = alpha_step(model, x)\n",
    "    beta = beta_step(model, x)\n",
    "    gamma, evd = compute_gamma(alpha, beta)\n",
    "    zeta = compute_zeta(model, alpha, beta, evd, x)\n",
    "    return gamma, evd, zeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_via_em(data, model, n_epochs, verbose=False, evd_tolerance=1e-4):\n",
    "    \n",
    "    X = data.copy()\n",
    "    model = deepcopy(model)\n",
    "    prev_evd = float('-inf')\n",
    "    \n",
    "    for n in range(n_epochs):\n",
    "        # E-step (compute posteriors)\n",
    "        gamma, evd, zeta = zip(*[e_step(model, x) for x in X])\n",
    "        evd = np.sum(evd)\n",
    "\n",
    "        # M-step (update parameters)\n",
    "        ## Pi\n",
    "        model.pi = sum(gamma)[0] / sum(gamma)[0].sum()\n",
    "        ## A\n",
    "        zeta = sum([zta.sum(0) for zta in zeta])\n",
    "        model.A = zeta / zeta.sum(1)[:, None]\n",
    "        ## mu\n",
    "        for z in range(K):\n",
    "            model.mu[z] = sum((g[:, z].numpy() * x).sum() for g, x in zip(gamma, X)) / sum(g[:, z].sum() for g in gamma)\n",
    "\n",
    "        assert np.allclose(model.pi.sum(), 1.)\n",
    "        assert np.allclose(model.A.sum(1), 1.)\n",
    "        assert evd >= (prev_evd - evd_tolerance)\n",
    "\n",
    "        if (n % (n_epochs / 10) == 0 and n != 0) or verbose:\n",
    "            print(f'Epoch {n} | P(X): {evd:1.5}')\n",
    "\n",
    "        prev_evd = evd\n",
    "        \n",
    "    gamma, evd, zeta = zip(*[e_step(model, x) for x in X])\n",
    "    evd = np.sum(evd)\n",
    "        \n",
    "    return model, evd, gamma, zeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-sum (Viterbi algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_sum_messages(model, x):\n",
    "    omega = []\n",
    "\n",
    "    for m in range(M):\n",
    "        if m == 0:\n",
    "            o_m = [model.pi[z].log() + model.loglik(z, x[m]) for z in range(K)]\n",
    "            o_m_idx = [None] * len(o_m)\n",
    "        else:\n",
    "            o_m = []\n",
    "            o_m_idx = []\n",
    "            for z_j in range(K):\n",
    "                ll = model.loglik(z_j, x[m])\n",
    "                mx, mx_idx = torch.tensor([ll + model.A[z_i][z_j].log() + omega[-1][z_i][0] for z_i in range(K)]).max(0)\n",
    "                o_m.append(mx)\n",
    "                o_m_idx.append(mx_idx)\n",
    "        o = [(t.item(), i.item()) if i is not None else (t.item(), i) for t, i in zip(o_m, o_m_idx)]\n",
    "        omega.append(o)\n",
    "    return omega\n",
    "\n",
    "\n",
    "def backtrack(omega):\n",
    "    m = M\n",
    "    configs = []\n",
    "    while omega:\n",
    "        o_m = omega.pop()\n",
    "        vals, idxs = zip(*o_m)\n",
    "        if M == 1:\n",
    "            max_val = max(vals)\n",
    "            configs.append([vals.index(max_val)])\n",
    "        elif all([i is not None for i in idxs]):\n",
    "            if m == M:\n",
    "                max_val = max(vals)\n",
    "                for i, v in enumerate(vals):\n",
    "                    if v == max_val:\n",
    "                        c = deque([idxs[i], i])\n",
    "                        configs.append(c)\n",
    "            else:\n",
    "                for c in configs:\n",
    "                    phi = c[0]\n",
    "                    c.appendleft(idxs[phi])\n",
    "            m -= 1\n",
    "    configs = [tuple(c) for c in configs]\n",
    "    return max_val, configs\n",
    "\n",
    "\n",
    "def viterbi(model, x):\n",
    "    omega = compute_max_sum_messages(model, x)\n",
    "    max_val, configs = backtrack(omega)\n",
    "    return max_val, configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X[0]\n",
    "\n",
    "# Theirs\n",
    "dists = [NormalDistribution(m, sd) for m, sd in zip(mu, std)]\n",
    "trans_mat = A.numpy()\n",
    "starts = pi.numpy()\n",
    "their_model = HiddenMarkovModel.from_matrix(trans_mat, dists, starts)\n",
    "\n",
    "# Ours\n",
    "our_model = HMM(pi=pi, A=A, mu=mu, std=std)\n",
    "\n",
    "# Test\n",
    "\n",
    "## E-step\n",
    "gamma, evd, zeta = e_step(our_model, x)\n",
    "\n",
    "### Log-prob\n",
    "assert np.allclose(np.exp(their_model.log_probability(x)), evd.item())\n",
    "\n",
    "### Transitions, emissions\n",
    "transitions, emissions = their_model.forward_backward(x)\n",
    "assert np.allclose(np.exp(emissions), gamma)\n",
    "assert np.allclose(transitions[:K, :K], zeta.sum(0))\n",
    "\n",
    "## Viterbi\n",
    "their_max_val, their_states = their_model.viterbi(x)\n",
    "their_states, _ = zip(*their_states[1:])\n",
    "our_max_val, our_states = viterbi(our_model, x)\n",
    "assert their_states in our_states\n",
    "assert np.allclose(their_max_val, our_max_val)\n",
    "\n",
    "## M-step\n",
    "assert np.allclose(our_model.pi.numpy(), their_model.dense_transition_matrix()[their_model.start_index, :][:K])\n",
    "assert np.allclose(our_model.A.numpy(), their_model.dense_transition_matrix()[:K, :K])\n",
    "_ = their_model.fit(X, min_iterations=1, max_iterations=1, algorithm='baum-welch', stop_threshold=1e-15)\n",
    "mdl, evd, gamma, zeta = train_via_em(data=X, model=our_model, n_epochs=1)\n",
    "assert np.allclose(mdl.pi, their_model.dense_transition_matrix()[their_model.start_index, :][:K])\n",
    "assert np.allclose(mdl.A, their_model.dense_transition_matrix()[:K, :K])\n",
    "their_mu, their_std = zip(*[s.distribution.parameters for s in their_model.get_params()['states'][:K]])\n",
    "assert np.allclose(their_mu, [m.item() for m in mdl.mu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | P(X): 5.1061e-05\n",
      "Epoch 20 | P(X): 8.383e-05\n",
      "Epoch 30 | P(X): 0.00010088\n",
      "Epoch 40 | P(X): 0.00010554\n",
      "Epoch 50 | P(X): 0.0001061\n",
      "Epoch 60 | P(X): 0.00010601\n",
      "Epoch 70 | P(X): 0.00010601\n",
      "Epoch 80 | P(X): 0.00010599\n",
      "Epoch 90 | P(X): 0.00010592\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "model = HMM()\n",
    "\n",
    "model, evd, gamma, zeta = train_via_em(X, model, N_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuassianObservationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, K, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(K, embed_dim)\n",
    "        self.hidden = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "        self.mu = nn.Parameter(torch.ones(K))\n",
    "        \n",
    "    def forward(self, z):\n",
    "        embed = self.embed(torch.LongTensor([z]))\n",
    "        return torch.relu(self.out(self.hidden(embed)))  # ReLU, b/c we know obs. means are positive\n",
    "    \n",
    "    \n",
    "class NeuralHMM(nn.Module):\n",
    "    \n",
    "    EMBED_DIM = 10\n",
    "    HIDDEN_DIM = 5\n",
    "    OBS_MODEL_STD = 1.\n",
    "    \n",
    "    def __init__(self, pi=None, A=None, mu=None, std=None, seed=42):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self._A = A if A is not None else nn.Parameter(torch.randn(K, K))\n",
    "        self._pi = pi if pi is not None else nn.Parameter(torch.randn(K))\n",
    "        self.obs_model = GuassianObservationModel(K, self.EMBED_DIM, self.HIDDEN_DIM)\n",
    "        \n",
    "    @property\n",
    "    def pi(self):\n",
    "        return F.softmax(self._pi, dim=-1)\n",
    "    \n",
    "    @property\n",
    "    def A(self):\n",
    "        return F.softmax(self._A, dim=-1)\n",
    "        \n",
    "    def loglik(self, z, x):\n",
    "        \"\"\"\n",
    "        The likelihood of the given observation `x`, conditional on latent code `z`, \n",
    "        given a Gaussian observation model.\n",
    "        \"\"\"\n",
    "        x = torch.FloatTensor([x])\n",
    "        return Normal(loc=self.obs_model(z), scale=self.OBS_MODEL_STD).log_prob(x)\n",
    "    \n",
    "    def factor(self, z_i, z_j, x):\n",
    "        return self.A[z_i][z_j] * self.loglik(z_j, x).exp()\n",
    "    \n",
    "    def elbo(self, X, gamma, zeta):\n",
    "        Q = 0\n",
    "        for x in X:\n",
    "            Q += (gamma[0] * self.pi.log()).sum()\n",
    "            for zta in zeta:\n",
    "                Q += (zta * self.A.log()).sum()\n",
    "            for gma in gamma:\n",
    "                for z in range(K):\n",
    "                    Q += (gma[z] * self.loglik(z, x)).sum()\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | P(X): 1.3345e-05 | ELBO: -16584.544921875\n",
      "Epoch 20 | P(X): 2.2306e-05 | ELBO: -15657.876953125\n",
      "Epoch 30 | P(X): 2.6937e-05 | ELBO: -15233.6865234375\n",
      "Epoch 40 | P(X): 2.9675e-05 | ELBO: -15065.7841796875\n",
      "Epoch 50 | P(X): 3.0983e-05 | ELBO: -14989.5029296875\n",
      "Epoch 60 | P(X): 3.1971e-05 | ELBO: -14914.7099609375\n",
      "Epoch 70 | P(X): 3.2353e-05 | ELBO: -14827.8447265625\n",
      "Epoch 80 | P(X): 3.2619e-05 | ELBO: -14772.3232421875\n",
      "Epoch 90 | P(X): 3.2909e-05 | ELBO: -14744.5869140625\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "LR = .05\n",
    "VERBOSE = False\n",
    "ALPHA = 1\n",
    "\n",
    "model = NeuralHMM()\n",
    "optim = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for n in range(N_EPOCHS):\n",
    "    gamma, evd, zeta = zip(*[e_step(model, x) for x in X])\n",
    "    gamma, zeta, evd = sum(gamma), sum(zeta), sum(evd)\n",
    "    Q = model.elbo(X, gamma, zeta)\n",
    "    optim.zero_grad()\n",
    "    (-Q).backward()\n",
    "    optim.step()\n",
    "    \n",
    "    assert np.allclose(model.pi.sum().detach().numpy(), 1.)\n",
    "    assert np.allclose(model.A.sum(1).detach().numpy(), 1.)\n",
    "    \n",
    "    if n % (N_EPOCHS / 10) == 0 and n != 0 or VERBOSE:\n",
    "        print(f'Epoch {n} | P(X): {evd:1.5} | ELBO: {Q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralHMMWithTriuDirichletPrior(NeuralHMM):\n",
    "    \n",
    "    def __init__(self, concentration_matrix, pi=None, A=None, mu=None, std=None, seed=42):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.prior = Dirichlet(torch.FloatTensor(concentration_matrix))\n",
    "        self._A = A if A is not None else nn.Parameter(torch.randn(K, K))\n",
    "        self._pi = pi if pi is not None else nn.Parameter(torch.randn(K))\n",
    "        self.obs_model = GuassianObservationModel(K, self.EMBED_DIM, self.HIDDEN_DIM)\n",
    "    \n",
    "    def elbo(self, X, gamma, zeta, eps=1e-30):\n",
    "        Q = super().elbo(X, gamma, zeta)\n",
    "        # pi\n",
    "        jac = []\n",
    "        for v in self.pi:\n",
    "            v.backward(retain_graph=True)\n",
    "            jac.append(self._pi.grad)\n",
    "        jac = torch.stack(jac, dim=-1)\n",
    "        Q += self.prior.log_prob(self.pi + eps).sum() + (jac.det().abs() + eps).log()\n",
    "        # A\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3544, 0.0488, 0.0933, 0.4617, 0.0417], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3635, 0.0527, 0.0986, 0.4372, 0.0480], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3786, 0.0555, 0.1022, 0.4089, 0.0547], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4031, 0.0576, 0.1011, 0.3768, 0.0614], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4297, 0.0610, 0.0979, 0.3433, 0.0682], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4564, 0.0651, 0.0935, 0.3099, 0.0751], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4823, 0.0692, 0.0881, 0.2782, 0.0822], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5059, 0.0724, 0.0825, 0.2496, 0.0896], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5255, 0.0749, 0.0771, 0.2249, 0.0977], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5410, 0.0765, 0.0720, 0.2038, 0.1067], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5530, 0.0773, 0.0671, 0.1859, 0.1167], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 10 | P(X): 1.8108e-05 | ELBO: -16019.5859375\n",
      "tensor([0.5607, 0.0773, 0.0628, 0.1711, 0.1281], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5625, 0.0770, 0.0592, 0.1596, 0.1417], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5576, 0.0767, 0.0567, 0.1510, 0.1580], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5465, 0.0765, 0.0550, 0.1446, 0.1774], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5308, 0.0759, 0.0539, 0.1395, 0.1999], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.5120, 0.0750, 0.0528, 0.1350, 0.2252], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4913, 0.0736, 0.0515, 0.1308, 0.2528], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4690, 0.0719, 0.0501, 0.1267, 0.2824], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4450, 0.0697, 0.0484, 0.1226, 0.3142], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.4191, 0.0672, 0.0468, 0.1184, 0.3485], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 20 | P(X): 3.2336e-05 | ELBO: -15080.72265625\n",
      "tensor([0.3916, 0.0641, 0.0452, 0.1139, 0.3852], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3630, 0.0607, 0.0437, 0.1091, 0.4235], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3342, 0.0570, 0.0423, 0.1039, 0.4626], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.3061, 0.0531, 0.0412, 0.0984, 0.5012], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2798, 0.0493, 0.0406, 0.0927, 0.5376], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2568, 0.0456, 0.0410, 0.0870, 0.5696], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2378, 0.0423, 0.0422, 0.0817, 0.5960], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2224, 0.0395, 0.0443, 0.0768, 0.6170], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.2100, 0.0372, 0.0470, 0.0725, 0.6333], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1997, 0.0352, 0.0505, 0.0687, 0.6459], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 30 | P(X): 2.7818e-05 | ELBO: -14813.625\n",
      "tensor([0.1911, 0.0335, 0.0548, 0.0654, 0.6552], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1836, 0.0321, 0.0601, 0.0624, 0.6618], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1766, 0.0308, 0.0662, 0.0597, 0.6667], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1696, 0.0296, 0.0730, 0.0571, 0.6708], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1624, 0.0283, 0.0801, 0.0547, 0.6744], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1550, 0.0271, 0.0875, 0.0524, 0.6780], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1473, 0.0259, 0.0951, 0.0501, 0.6815], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1395, 0.0246, 0.1026, 0.0479, 0.6854], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1317, 0.0234, 0.1095, 0.0457, 0.6897], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1240, 0.0221, 0.1155, 0.0437, 0.6947], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 40 | P(X): 3.3596e-05 | ELBO: -14615.107421875\n",
      "tensor([0.1164, 0.0209, 0.1203, 0.0417, 0.7006], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1092, 0.0198, 0.1237, 0.0398, 0.7076], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.1023, 0.0187, 0.1255, 0.0380, 0.7156], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0957, 0.0176, 0.1256, 0.0362, 0.7249], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0896, 0.0166, 0.1240, 0.0346, 0.7352], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0839, 0.0156, 0.1209, 0.0331, 0.7465], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0787, 0.0148, 0.1164, 0.0317, 0.7585], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0739, 0.0139, 0.1108, 0.0303, 0.7710], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0695, 0.0132, 0.1046, 0.0291, 0.7837], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0655, 0.0125, 0.0980, 0.0279, 0.7961], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 50 | P(X): 3.1209e-05 | ELBO: -14558.4150390625\n",
      "tensor([0.0618, 0.0118, 0.0913, 0.0269, 0.8082], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0585, 0.0112, 0.0847, 0.0259, 0.8197], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0555, 0.0107, 0.0783, 0.0250, 0.8305], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0528, 0.0102, 0.0724, 0.0241, 0.8405], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0503, 0.0097, 0.0669, 0.0233, 0.8497], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0480, 0.0093, 0.0618, 0.0226, 0.8582], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0459, 0.0089, 0.0572, 0.0219, 0.8660], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0440, 0.0086, 0.0531, 0.0213, 0.8731], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0422, 0.0082, 0.0493, 0.0207, 0.8796], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0405, 0.0079, 0.0458, 0.0201, 0.8856], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 60 | P(X): 3.1869e-05 | ELBO: -14520.1845703125\n",
      "tensor([0.0390, 0.0076, 0.0427, 0.0196, 0.8911], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0375, 0.0073, 0.0399, 0.0190, 0.8963], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0361, 0.0071, 0.0374, 0.0185, 0.9010], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0347, 0.0068, 0.0350, 0.0181, 0.9054], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0335, 0.0066, 0.0329, 0.0176, 0.9094], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0323, 0.0063, 0.0310, 0.0172, 0.9132], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0312, 0.0061, 0.0293, 0.0167, 0.9167], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0301, 0.0059, 0.0277, 0.0163, 0.9200], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0291, 0.0057, 0.0263, 0.0159, 0.9230], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0281, 0.0056, 0.0249, 0.0156, 0.9258], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 70 | P(X): 3.2434e-05 | ELBO: -14504.09765625\n",
      "tensor([0.0273, 0.0054, 0.0238, 0.0152, 0.9284], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0264, 0.0052, 0.0227, 0.0149, 0.9308], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0256, 0.0051, 0.0217, 0.0145, 0.9331], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0249, 0.0049, 0.0208, 0.0142, 0.9351], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0242, 0.0048, 0.0200, 0.0139, 0.9371], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0235, 0.0047, 0.0192, 0.0137, 0.9389], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0229, 0.0046, 0.0186, 0.0134, 0.9405], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0224, 0.0045, 0.0179, 0.0132, 0.9421], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0218, 0.0044, 0.0174, 0.0129, 0.9435], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0213, 0.0043, 0.0168, 0.0127, 0.9449], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 80 | P(X): 3.1836e-05 | ELBO: -14494.8046875\n",
      "tensor([0.0208, 0.0042, 0.0164, 0.0125, 0.9461], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0204, 0.0041, 0.0159, 0.0123, 0.9473], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0200, 0.0040, 0.0155, 0.0121, 0.9484], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0196, 0.0039, 0.0151, 0.0119, 0.9495], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0192, 0.0038, 0.0147, 0.0118, 0.9505], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0188, 0.0038, 0.0144, 0.0116, 0.9514], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0185, 0.0037, 0.0141, 0.0114, 0.9523], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0181, 0.0036, 0.0138, 0.0113, 0.9532], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0178, 0.0036, 0.0134, 0.0111, 0.9541], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0175, 0.0035, 0.0132, 0.0109, 0.9549], grad_fn=<SoftmaxBackward>)\n",
      "Epoch 90 | P(X): 3.219e-05 | ELBO: -14489.1884765625\n",
      "tensor([0.0172, 0.0035, 0.0129, 0.0108, 0.9557], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0169, 0.0034, 0.0126, 0.0106, 0.9565], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0166, 0.0033, 0.0124, 0.0105, 0.9572], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0163, 0.0033, 0.0121, 0.0104, 0.9580], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0160, 0.0032, 0.0119, 0.0102, 0.9587], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0158, 0.0032, 0.0116, 0.0101, 0.9594], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0155, 0.0031, 0.0114, 0.0099, 0.9600], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0152, 0.0031, 0.0112, 0.0098, 0.9607], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0150, 0.0030, 0.0110, 0.0097, 0.9613], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "LR = .1\n",
    "VERBOSE = False\n",
    "# CONCENTRATION_MATRIX = torch.ones(K, K) + torch.randint(0, 5, (K, K)).float().triu()\n",
    "CONCENTRATION_MATRIX = torch.FloatTensor([1, 1, 1, 1, 10])\n",
    "\n",
    "model = NeuralHMMWithTriuDirichletPrior(CONCENTRATION_MATRIX)\n",
    "optim = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for n in range(N_EPOCHS):\n",
    "    gamma, evd, zeta = zip(*[e_step(model, x) for x in X])\n",
    "    gamma, zeta, evd = sum(gamma), sum(zeta), sum(evd)\n",
    "    Q = model.elbo(X, gamma, zeta)\n",
    "    optim.zero_grad()\n",
    "    (-Q).backward()\n",
    "    optim.step()\n",
    "    \n",
    "    assert np.allclose(model.pi.sum().detach().numpy(), 1.)\n",
    "    assert np.allclose(model.A.sum(1).detach().numpy(), 1.)\n",
    "    \n",
    "    print(model.pi)\n",
    "    \n",
    "    if n % (N_EPOCHS / 10) == 0 and n != 0 or VERBOSE:\n",
    "        print(f'Epoch {n} | P(X): {evd:1.5} | ELBO: {Q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(nn.Module):\n",
    "    \n",
    "    A_prior = torch.FloatTensor([1, 1, 1, 1, 10])\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior = prior = Dirichlet(self.A_prior)\n",
    "        self._A = nn.Parameter(torch.randn(K))\n",
    "        \n",
    "    def forward(self, eps=1e-30):\n",
    "#         jac = -torch.ger(self.A, self.A) + torch.diag(self.A**2) + torch.diag(self.A * (1 - self.A))\n",
    "        jac = []\n",
    "        for v in self.A:\n",
    "            v.backward(retain_graph=True)\n",
    "            jac.append(self._A.grad)\n",
    "        jac = torch.stack(jac, dim=-1)\n",
    "            \n",
    "        return self.prior.log_prob(self.A + eps).sum() + (jac.det().abs() + eps).log()\n",
    "        \n",
    "    @property\n",
    "    def A(self):\n",
    "        return F.softmax(self._A, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt: -64.28128814697266\n",
      "tgt: -61.2588005065918\n",
      "tgt: -60.344970703125\n",
      "tgt: -59.96563720703125\n",
      "tgt: -59.770538330078125\n",
      "tgt: -59.65572738647461\n",
      "tgt: -59.58177947998047\n",
      "tgt: -59.5309944152832\n",
      "tgt: -59.494415283203125\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "LR = .01\n",
    "VERBOSE = False\n",
    "ALPHA = 1\n",
    "\n",
    "\n",
    "objective = Objective()\n",
    "optim = Adam(objective.parameters(), lr=LR)\n",
    "\n",
    "for n in range(N_EPOCHS):\n",
    "    optim.zero_grad()\n",
    "    tgt = objective()\n",
    "    (-tgt).backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if n % (N_EPOCHS / 10) == 0 and n != 0 or VERBOSE:\n",
    "        print(f'tgt: {tgt}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
